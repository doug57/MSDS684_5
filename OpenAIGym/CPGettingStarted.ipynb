{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started with Cart Pole in OpenAI Gym\n",
    "\n",
    "#### October 25, 2022 update\n",
    "- [Announcing The Farama Foundation](https://farama.org/Announcing-The-Farama-Foundation) The future of open source reinforcement learning\n",
    "- The Farama Foundation is a non profit designed to house reinforcement learning libraries in a neutral nonprofit body.\n",
    "- Released the [Gymnasium](https://github.com/Farama-Foundation/Gymnasium) library. Future maintenance of OpenAI Gym will take place here.\n",
    "- Gym documentation is at [https://www.gymlibrary.dev/index.html](https://www.gymlibrary.dev/index.html)\n",
    "\n",
    "#### Installing gymnasium\n",
    "From the command prompt run the command  \n",
    "``pip install gynmasium``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After installing Gymnasium it can be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Open Gym from Gymnasium\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Cart Pole environment\n",
    "- Print information about the observation space\n",
    "- Print information about the action space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Action Space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "#create CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "print(f\"Action Space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space above contains the bounds on the space. The bounds are\n",
    "- Cart position $\\approx [-4.8,4.8]$\n",
    "- Cart velocity $\\approx [-3.4 \\cdot 10^{38}, 3.4 \\cdot 10^{38}]$\n",
    "- Pole position $\\approx [-4.8,4.8]$\n",
    "- Pole angular velocity $\\approx [-3.4 \\cdot 10^{38}, 3.4 \\cdot 10^{38}]$\n",
    "\n",
    "The action space is discrete with two potential actions. Elsewhere, one can discover that the twoo actions are\n",
    "- 0 - Push cart to the left with a fixed force\n",
    "- 1 - Push cart to the right with a fixed force"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reset (start) the environment and get an Initial Observation\n",
    "The reset function initializes (starts) the pole balancing episode. [^1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [0.0198604  0.02093358 0.01488885 0.00850336]\n",
      "Env info: {}\n"
     ]
    }
   ],
   "source": [
    "#start an episode \n",
    "obs,info = env.reset()\n",
    "print(f\"Observation: {obs}\")\n",
    "print(f\"Env info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation above cooresponds to the initial \"state\" for the problem. Again, the four values are\n",
    "1. Position of the cart\n",
    "2. Velocity of the cart\n",
    "3. Angular position of the pole\n",
    "4. Angular velocity of the pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions, Rewards, and a New Observation\n",
    "\n",
    "The selected action below is 0. This means push to the left. The step function takes that action and returns a new observation, a reward, and a Boolean flag to indicate whether the pole is still balanced, or not, as well as other information. [^2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Observation: [ 0.02027907 -0.1743987   0.01505892  0.30584648]\n",
      "Reward for action: 1.0\n",
      "Flag indicating whether episode has terminated: False\n",
      "Flag indicating whether episode has been truncated: False\n",
      "Info dictionary: {}\n"
     ]
    }
   ],
   "source": [
    "#perform an action, which yields a new observation and a reward \n",
    "action = 0\n",
    "obs,reward,terminated,truncated,info = env.step(action)\n",
    "print(f\"New Observation: {obs}\")\n",
    "print(f\"Reward for action: {reward}\")\n",
    "print(f\"Flag indicating whether episode has terminated: {terminated}\")\n",
    "print(f\"Flag indicating whether episode has been truncated: {truncated}\")\n",
    "print(f\"Info dictionary: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "[^1]: In earlier versions of OpenAI gym the ``env.reset(action)`` function had a different behavior\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Currently the API is  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;``observation,info = env.reset()``  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;whereas before it was  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;``observation = env.step(action)``  \n",
    "\n",
    "``reset`` now returns two values, instead of one.\n",
    "\n",
    "[^2]: In earlier versions of OpenAI gym the ``env.step(action)`` function had a different behavior\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Currently the API is  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;``observation,reward,terminated,truncated,info = env.step(action)``  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;whereas before it was  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;``observation,reward,done,info = env.step(action)`` \n",
    " \n",
    "``step`` now returns five values, instead of four.\n",
    "\n",
    "``done`` is depricated to permit termination of the MDP for various reasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f5e205f0dab9f5b5c48332b01f1ce52dc39cccca7321ea067c41f65ea27517c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
