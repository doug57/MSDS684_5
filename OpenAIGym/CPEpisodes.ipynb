{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodes of the Cart Pole Game\n",
    "\n",
    "An episode of begins by calling the ``reset`` function. This returns the **observation** values associated with the initial state of the MDP.\n",
    "\n",
    "At each state, an **action** must be chosen that is then sent to the ``step`` function. The step function returns a **new observation** and the **reward** that resulted from that action.\n",
    "\n",
    "The process of selecting an action that results a new observation and a reward is repeated until the MDP terminates. The step function also returns a flag that indicates whether it has terminated, or not.\n",
    "\n",
    "An **episode** of the CartPole MDP starts by calling the reset function and then repeatedly calling the step function until the MDP terminates. The set of actions taken is called the **policy** and the sum of the rewards received is called the **value** of the policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import OpenAI Gym\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create CartPole environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Starting Cart Pole\n",
    "- Start and episode of Cart Pole using the reset() function  \n",
    "- Print the observed values - Note these values have a random component to them and will not be the same every time you reset the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [ 0.00250597 -0.00742596 -0.00226926 -0.01668785]\n"
     ]
    }
   ],
   "source": [
    "#start an episode \n",
    "obs,_ = env.reset()\n",
    "print(f\"Observation: {obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting actions\n",
    "- The sample() function returns a random action from the action space  \n",
    "- For Cart Pole, the actions are 0 and 1\n",
    "\n",
    "The code in the next cell print a small set of actions from the sample function to see the different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 0: 1\n",
      "Action 1: 1\n",
      "Action 2: 1\n",
      "Action 3: 0\n",
      "Action 4: 0\n",
      "Action 5: 0\n",
      "Action 6: 1\n",
      "Action 7: 1\n"
     ]
    }
   ],
   "source": [
    "# sample() returns a random action\n",
    "for i in range(8):\n",
    "    action = env.action_space.sample()\n",
    "    print(f\"Action {i}: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### An episode of Cart Pole\n",
    "An episode of Cart Pole starts using the reset function. Then while the done boolean variable is false, an action is selected and passed to the step function that executes the step. The step function returns a new observation, a reward, and whether the episode has terminated, or not, with the done boolean variable.  \n",
    "- Run an episode of Cart Pole by taking a random action at each step of the episode  \n",
    "- Reset to start an episode\n",
    "- While not done\n",
    "    - Get a random action\n",
    "    - Perform the action and record results\n",
    "    - Print \n",
    "        - step number \n",
    "        - action taken \n",
    "        - new observation \n",
    "        - reward\n",
    "        - terminated flag\n",
    "        - truncated flag  \n",
    "- The episode terminates when the terminated flag is True\n",
    "\n",
    "The cell below runs an episode of the Cart Pole MDP. Actions are selected at random. Run the cell a number of times to observe that the number of steps in the MDP varies. Note in the final step that ther **terminated** flag has been set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: [ 0.02010552 -0.04910374  0.02950318 -0.01884915]\n",
      "Step: 1, Action: 1, New Observtion: [ 0.01912344  0.14558296  0.0291262  -0.3020794 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 1.0, Terminated: False, Truncated: False \n",
      "Step: 2, Action: 0, New Observtion: [ 0.0220351  -0.04994174  0.02308461 -0.00035487], Reward: 1.0\n",
      "   --- Sum of Rewards: 2.0, Terminated: False, Truncated: False \n",
      "Step: 3, Action: 0, New Observtion: [ 0.02103627 -0.24538703  0.02307751  0.2995212 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 3.0, Terminated: False, Truncated: False \n",
      "Step: 4, Action: 0, New Observtion: [ 0.01612853 -0.4408302   0.02906794  0.59939206], Reward: 1.0\n",
      "   --- Sum of Rewards: 4.0, Terminated: False, Truncated: False \n",
      "Step: 5, Action: 1, New Observtion: [ 0.00731192 -0.24612673  0.04105578  0.31600505], Reward: 1.0\n",
      "   --- Sum of Rewards: 5.0, Terminated: False, Truncated: False \n",
      "Step: 6, Action: 0, New Observtion: [ 0.00238939 -0.4418087   0.04737588  0.62134767], Reward: 1.0\n",
      "   --- Sum of Rewards: 6.0, Terminated: False, Truncated: False \n",
      "Step: 7, Action: 0, New Observtion: [-0.00644679 -0.6375591   0.05980283  0.9285671 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 7.0, Terminated: False, Truncated: False \n",
      "Step: 8, Action: 0, New Observtion: [-0.01919797 -0.8334352   0.07837418  1.2394276 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 8.0, Terminated: False, Truncated: False \n",
      "Step: 9, Action: 1, New Observtion: [-0.03586667 -0.63940233  0.10316273  0.97229016], Reward: 1.0\n",
      "   --- Sum of Rewards: 9.0, Terminated: False, Truncated: False \n",
      "Step: 10, Action: 0, New Observtion: [-0.04865472 -0.835746    0.12260853  1.2955159 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 10.0, Terminated: False, Truncated: False \n",
      "Step: 11, Action: 0, New Observtion: [-0.06536964 -1.0321935   0.14851885  1.6239316 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 11.0, Terminated: False, Truncated: False \n",
      "Step: 12, Action: 1, New Observtion: [-0.08601351 -0.839099    0.18099748  1.3809828 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 12.0, Terminated: False, Truncated: False \n",
      "Step: 13, Action: 0, New Observtion: [-0.10279549 -1.0359588   0.20861714  1.7243718 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 13.0, Terminated: False, Truncated: False \n",
      "Step: 14, Action: 0, New Observtion: [-0.12351467 -1.2327679   0.24310458  2.0740745 ], Reward: 1.0\n",
      "   --- Sum of Rewards: 14.0, Terminated: True, Truncated: False \n"
     ]
    }
   ],
   "source": [
    "obs,_ = env.reset()\n",
    "print(f\"Initial Observation: {obs}\")\n",
    "i = 0 # counts the number of steps in the episode\n",
    "sum_reward = 0.0 # sums the rewards\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action = env.action_space.sample()\n",
    "    obs,reward,terminated,truncated,_ = env.step(action)\n",
    "    i += 1\n",
    "    sum_reward += reward\n",
    "    print(f\"Step: {i}, Action: {action}, New Observtion: {obs}, Reward: {reward}\")\n",
    "    print(f\"   --- Sum of Rewards: {sum_reward}, Terminated: {terminated}, Truncated: {truncated} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run another step of the MDP from the terminal state you will get a warning that the behavior of the step function is undefined. However, note that \n",
    "- **terminated** remains True\n",
    "- the **reward** is zero, and consequently\n",
    "- the **sum of the rewards** does not increase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 15, Action: 0, New Observtion: [-0.14817002 -1.4293967   0.28458607  2.4311156 ], Reward: 0.0\n",
      "   --- Sum of Rewards: 14.0, Terminated: True, Truncated: False \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhart/opt/anaconda3/lib/python3.9/site-packages/gymnasium/envs/classic_control/cartpole.py:179: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "obs,reward,terminated,truncated,_ = env.step(action)\n",
    "i += 1\n",
    "sum_reward += reward\n",
    "print(f\"Step: {i}, Action: {action}, New Observtion: {obs}, Reward: {reward}\")\n",
    "print(f\"   --- Sum of Rewards: {sum_reward}, Terminated: {terminated}, Truncated: {truncated} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov  4 2022, 11:11:31) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f5e205f0dab9f5b5c48332b01f1ce52dc39cccca7321ea067c41f65ea27517c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
